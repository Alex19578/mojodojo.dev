{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Benchmark\n",
    "categories: Benchmark\n",
    "usage: Pass in a closure that returns None as a parameter to benchmark its speed in nanoseconds\n",
    "---\n",
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Benchmark import Benchmark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through each number up to `n` and calculate the total in the fibonacci sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias n = 35 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the recursive version first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn fib(n: Int) -> Int:\n",
    "    if n <= 1:\n",
    "       return n \n",
    "    else:\n",
    "       return fib(n-1) + fib(n-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To benchmark it, create a nested `fn` that takes no arguments and doesn't return anything, then pass it in as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nanoseconds: 49788877\n",
      "Seconds: 0.049788877000000002\n"
     ]
    }
   ],
   "source": [
    "fn bench():\n",
    "    fn closure():\n",
    "        for i in range(n):\n",
    "            _ = fib(i)\n",
    "\n",
    "    let recursive = Benchmark().run[closure]()\n",
    "    print(\"Nanoseconds:\", recursive)\n",
    "    print(\"Seconds:\", F64(recursive) / 1e9)\n",
    "\n",
    "bench()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define iterative version for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nanoseconds iterative: 0\n"
     ]
    }
   ],
   "source": [
    "fn fib_iterative(n: Int) -> Int:\n",
    "    var count = 0\n",
    "    var n1 = 0\n",
    "    var n2 = 1\n",
    "\n",
    "    while count < n:\n",
    "       let nth = n1 + n2\n",
    "       n1 = n2\n",
    "       n2 = nth\n",
    "       count += 1\n",
    "    return n1\n",
    "\n",
    "fn bench_iterative():\n",
    "    fn iterative_closure():\n",
    "        for i in range(n):\n",
    "            _ = fib_iterative(i)\n",
    "\n",
    "    let iterative = Benchmark().run[iterative_closure]()\n",
    "    print(\"Nanoseconds iterative:\", iterative)\n",
    "\n",
    "bench_iterative()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the compiler has optimized away everything, LLVM can change an iterative loop to a constant value if all the inputs are known at compile time through `constant folding`, or if the value isn't actually used for anything with `Dead Code Elimination` both of which could be occurring here.\n",
    "\n",
    "There is a lot going on under the hood, and so you should always test your assumptions with benchmarks, especially if you're adding complexity because you _think_ it will improve performance, [which often isn't the case](https://vimeo.com/649009599)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Benchmark` has a few different arguments, make a simple benchmark to see what they all do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 warmup iters, 5 max iters, 0 min time, 100_000_000ns max time\n",
      "sleeping 300,000ns\n",
      "sleeping 300,000ns\n",
      "sleeping 300,000ns\n",
      "sleeping 300,000ns\n",
      "sleeping 300,000ns\n",
      "sleeping 300,000ns\n",
      "average time 359957\n",
      "\n",
      "0 warmup iters, 5 max iters, 0 min time, 1_000_000ns max time\n",
      "sleeping 300,000ns\n",
      "sleeping 300,000ns\n",
      "sleeping 300,000ns\n",
      "average time 359248\n"
     ]
    }
   ],
   "source": [
    "from Time import sleep\n",
    "\n",
    "fn bench_args():\n",
    "    fn sleeper():\n",
    "        print(\"sleeping 300,000ns\")\n",
    "        sleep(3e-4)\n",
    "    \n",
    "    print(\"0 warmup iters, 5 max iters, 0 min time, 1_000_000_000ns max time\")\n",
    "    var nanoseconds = Benchmark(0, 5, 0, 100_000_000).run[sleeper]()\n",
    "    print(\"average time\", nanoseconds)\n",
    "\n",
    "    # Limit the max running time, so it never goes over 1 second\n",
    "    print(\"\\n0 warmup iters, 5 max iters, 0 min time, 1_000_000ns max time\")\n",
    "    nanoseconds = Benchmark(0, 5, 0, 1_000_000).run[sleeper]()\n",
    "    print(\"average time\", nanoseconds)\n",
    "\n",
    "bench_args()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note above, the target max iters was 5, it printed 6 iterations as there is a little extra logic inside `Benchmark.run`.\n",
    "\n",
    "On the second run we set the max to 1,000,000ns so it only has time to run 3 iterations before it finishes.\n",
    "\n",
    "Try one more with a warmup, and set the minimum to 3 million nanoseconds, which will mean it ignores the max iterations and runs 1 warmup and 9 normal runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeping 300,000ns\n",
      "sleeping 300,000ns\n",
      "sleeping 300,000ns\n",
      "sleeping 300,000ns\n",
      "sleeping 300,000ns\n",
      "sleeping 300,000ns\n",
      "sleeping 300,000ns\n",
      "sleeping 300,000ns\n",
      "sleeping 300,000ns\n",
      "sleeping 300,000ns\n",
      "average time 360254\n"
     ]
    }
   ],
   "source": [
    "fn bench_args():\n",
    "    fn sleeper():\n",
    "        print(\"sleeping 300,000ns\")\n",
    "        sleep(3e-4)\n",
    "\n",
    "    let nanoseconds = Benchmark(1, 2, 3_000_000, 1_000_000_000).run[sleeper]()\n",
    "    print(\"average time\", nanoseconds)\n",
    "\n",
    "bench_args()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mojo",
   "language": "mojo",
   "name": "mojo-jupyter-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "mojo"
   },
   "file_extension": ".mojo",
   "mimetype": "text/x-mojo",
   "name": "mojo"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
